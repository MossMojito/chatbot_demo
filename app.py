import streamlit as st
import faiss
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sentence_transformers import SentenceTransformer
import numpy as np

# --- Configuration ---
EMBEDDING_MODEL = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
LLM_MODEL = 'google/mt5-small'

# !!! EDIT THESE TWO VARIABLES !!!
FAISS_INDEX_PATH = "km1139171_index.faiss"  # <--- 1. Put your FAISS file name here
documents = [
    # <--- 2. Paste your list of documents here, exactly as used to create the index
    'Topic: à¹‚à¸›à¸£à¹‚à¸¡à¸Šà¸±à¹ˆà¸™à¸ à¸²à¸¢à¹ƒà¸™à¸›à¸£à¸°à¹€à¸—à¸¨ (à¸­à¸±à¸•à¸£à¸²/à¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚)',
    'Topic: à¹‚à¸›à¸£à¹‚à¸¡à¸Šà¸±à¹ˆà¸™à¸ à¸²à¸¢à¹ƒà¸™à¸›à¸£à¸°à¹€à¸—à¸¨ (à¸­à¸±à¸•à¸£à¸²/à¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚)',
    'Sub Topic: à¹à¸žà¹‡à¸à¹€à¸à¸ˆà¹€à¸ªà¸£à¸´à¸¡à¸£à¸²à¸¢à¹€à¸”à¸·à¸­à¸™ (Postpaid) à¸ªà¸¡à¸±à¸„à¸£à¸£à¸²à¸¢à¸„à¸£à¸±à¹‰à¸‡ (One-Time)',
    'Sub Topic: à¹à¸žà¹‡à¸à¹€à¸à¸ˆà¹€à¸ªà¸£à¸´à¸¡à¹€à¸•à¸´à¸¡à¹€à¸‡à¸´à¸™ (Prepaid) à¸ªà¸¡à¸±à¸„à¸£à¸£à¸²à¸¢à¸„à¸£à¸±à¹‰à¸‡ (One-Time)',
    'More Info1: à¹à¸žà¹‡à¸à¹€à¸ªà¸£à¸´à¸¡à¹€à¸™à¹‡à¸•',
    'More Info1: à¹à¸žà¹‡à¸à¹€à¸ªà¸£à¸´à¸¡à¹€à¸™à¹‡à¸•',
    # ... (paste all your other document strings here) ...
]
# !!! END OF EDITING SECTION !!!


@st.cache_resource
def load_models():
    """Loads the embedding and language models."""
    embedding_model = SentenceTransformer(EMBEDDING_MODEL)
    llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)
    llm_model = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL)
    return embedding_model, llm_tokenizer, llm_model

embedding_model, llm_tokenizer, llm_model = load_models()

try:
    faiss_index = faiss.read_index(FAISS_INDEX_PATH)
except Exception as e:
    st.error(f"Could not load the FAISS index file '{FAISS_INDEX_PATH}'. Make sure the file is in the same folder as app.py.")
    st.stop()

def answer_question(question, top_k=3):
    """Finds relevant docs and generates an answer using an LLM."""
    question_embedding = embedding_model.encode([question])
    distances, indices = faiss_index.search(np.array(question_embedding).astype('float32'), top_k)

    retrieved_docs = [documents[i] for i in indices[0]]
    context = " ".join(retrieved_docs)

    prompt = f"Based on this information: --- {context} --- Answer the question: {question}"

    input_ids = llm_tokenizer(prompt, return_tensors="pt", max_length=1024, truncation=True).input_ids
    outputs = llm_model.generate(input_ids, max_length=256, num_beams=5, early_stopping=True)
    answer = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)

    return answer, context

# --- Streamlit User Interface ---
st.title("ðŸ¤– Chatbot à¸–à¸²à¸¡-à¸•à¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹‚à¸›à¸£à¹‚à¸¡à¸Šà¸±à¹ˆà¸™")
st.write("à¸›à¹‰à¸­à¸™à¸„à¸³à¸–à¸²à¸¡à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸šà¹‚à¸›à¸£à¹‚à¸¡à¸Šà¸±à¹ˆà¸™ à¹à¸¥à¹‰à¸§ Chatbot à¸ˆà¸°à¸„à¹‰à¸™à¸«à¸²à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸¥à¸°à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¹ƒà¸«à¹‰à¸„à¸¸à¸“")

user_question = st.text_input("à¸„à¸³à¸–à¸²à¸¡à¸‚à¸­à¸‡à¸„à¸¸à¸“:")

if st.button("à¸ªà¹ˆà¸‡à¸„à¸³à¸–à¸²à¸¡"):
    if user_question:
        with st.spinner("à¸à¸³à¸¥à¸±à¸‡à¸„à¹‰à¸™à¸«à¸²à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸¥à¸°à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸³à¸•à¸­à¸š..."):
            answer, context = answer_question(user_question)
            st.subheader("à¸„à¸³à¸•à¸­à¸š:")
            st.write(answer)
            with st.expander("à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¹ƒà¸™à¸à¸²à¸£à¸•à¸­à¸š (Context)"):
                st.write(context)
    else:
        st.warning("à¸à¸£à¸¸à¸“à¸²à¸›à¹‰à¸­à¸™à¸„à¸³à¸–à¸²à¸¡")